Linear mixed model fit by REML. t-tests use Satterthwaite's method [
lmerModLmerTest]
Formula: PercT5 ~ -1 + Population + (1 | Line:Population) + (1 | Batch)
   Data: (subset(d_Pgm, Supervisor.PI == "Gibert"))

REML criterion at convergence: 13270.3

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.9328 -0.6447 -0.0768  0.5422  5.7227 

Random effects:
 Groups          Name        Variance Std.Dev.
 Line:Population (Intercept)  36.17    6.014  
 Batch           (Intercept)   0.00    0.000  
 Residual                    105.55   10.274  
Number of obs: 1741, groups:  Line:Population, 167; Batch, 3

Fixed effects:
             Estimate Std. Error      df t value Pr(>|t|)    
PopulationAK   32.603      1.525 156.954   21.39   <2e-16 ***
PopulationGI   29.152      1.757 155.855   16.59   <2e-16 ***
PopulationKA   29.325      1.524 156.586   19.25   <2e-16 ***
PopulationMA   36.393      1.568 158.331   23.20   <2e-16 ***
PopulationMU   34.539      1.525 156.954   22.66   <2e-16 ***
PopulationRE   30.902      1.702 156.037   18.16   <2e-16 ***
PopulationUM   32.690      1.605 156.342   20.36   <2e-16 ***
PopulationVA   32.058      1.563 156.470   20.51   <2e-16 ***
PopulationYE   33.204      1.541 162.629   21.55   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            PpltAK PpltGI PpltKA PpltMA PpltMU PpltRE PpltUM PpltVA
PopulatinGI 0.000                                                  
PopulatinKA 0.000  0.000                                           
PopulatinMA 0.000  0.000  0.000                                    
PopulatinMU 0.000  0.000  0.000  0.000                             
PopulatinRE 0.000  0.000  0.000  0.000  0.000                      
PopulatinUM 0.000  0.000  0.000  0.000  0.000  0.000               
PopulatinVA 0.000  0.000  0.000  0.000  0.000  0.000  0.000        
PopulatinYE 0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 
optimizer (nloptwrap) convergence code: 0 (OK)
boundary (singular) fit: see ?isSingular

